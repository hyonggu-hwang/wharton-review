[
  {
    "objectID": "day0.html",
    "href": "day0.html",
    "title": "Project Overview",
    "section": "",
    "text": "Day 1\n\nPlot the value function against the state variables. What is the intuition?\nPlot the policy function against the state variables. What is the intuition?\nHow do the two plots above change with the values of \\(\\alpha\\) and \\(\\delta\\)?\nWrite a subroutine that takes the model‚Äôs solution and parameters as inputs, then produces a simulated data set as output.\nUse the Tauchen (1986) method to discretize the state space.\n\n\n\nDay 2\n\nWhich features of the data will you ask the model to fit? These features may include means, variances, regression coefficients, or correlations.\nExplain which moment or moments are most important for identifying each model parameter. Create a table showing how each simulated moment changes as you perturb \\(\\alpha\\) and \\(\\delta\\).\nDownlad the data and read the documentation. The data set contains cleaned Compustat data on non-financial firms from 1971 - 2016. These variables should give you a hint about the types of moments that will work well.\nWrite a subroutine for computing the vector of moments from the actual data and simulated data.\nIf one of your moments is an AR1 coefficient, use the Han and Philips method at the end of Toni‚Äôs slides from today.\n\n\n\nDay 3\n\nUse influence functions to estimate the covariance matrix for the moments you have chosen, clustering by firm.\nCompute the optimal SMM weight matrix.\nCode up the SMM routine.\nWrite the code for finding the parameter vector that minimizes the econometric score, i.e.¬†the distance between the actual and simulated moments.\nChoose a minimizer that will avoid local minima, such as the simulated annealing minimization algorithm or particle swarm optimization.\n\n\n\nDay 4\n\nSMM estimator\nCompute your final parameter estimates and their standard errors. Do they make sense?\nHow well does the model fit the data? Compare the simulated and empirical moments.\nDoes the model fail the test of overidnentifying restrictions?\nWhat would be an interesting counterfactual experiment to run? How would you do it?"
  },
  {
    "objectID": "day3.html",
    "href": "day3.html",
    "title": "Day 3 - SMM",
    "section": "",
    "text": "Goals\nThe goals for this day are to code up the SMM estimator. That is, use influence functions to estimate the covariance matrix for the moments, compute the optimal weight matrix, code up the estimator, and debug it. The deliverables are:\n\nUse influence functions to estimate the covariance matrix for the moments you have chosen, clustering by firm.\nCompute the optimal SMM weight matrix.\nCode up the SMM routine.\nWrite the code for finding the parameter vector that minimizes the econometric score, i.e.¬†the distance between the actual and simulated moments.\nChoose a minimizer that will avoid local minima, such as the simulated annealing minimization algorithm or particle swarm optimization.\n\n\n\nSMM Estimator\n\n\nOptimizers\n\n\nWeighting Matrix"
  },
  {
    "objectID": "day4.html",
    "href": "day4.html",
    "title": "Day 4 - Overidentification Test",
    "section": "",
    "text": "Goals\nThe goals for this day are to finish coding up the SMM estimator, run it, and also perform the test of overidentifying restrictions and counterfactual analyses. The deliverables are:\n\nSMM estimator\nCompute your final parameter estimates and their standard errors. Do they make sense?\nHow well does the model fit the data? Compare the simulated and empirical moments.\nDoes the model fail the test of overidnentifying restrictions?\nWhat would be an interesting counterfactual experiment to run? How would you do it?\n\n\n\nStandard Errors\n\n\nOveridentification Test\n\n\nModel Fit\n\n\nCounterfactuals"
  },
  {
    "objectID": "functions.html",
    "href": "functions.html",
    "title": "Key Functions",
    "section": "",
    "text": "The goal of this section is to describe the core functions and document the way Toni organized her code. The functions can largely be divded into three categories. First, the model solver discretizes the state space, sets up the transition matrix, and solves the model. Second, the simulation subroutines simulate the model and collect statistics. Finally, the estimation prepares the data for estimation and runs the estimation routine.\nThese are some set-ups that will be necessary for the code to run.\n# Think about which file to include this code in eventually.\nusing Random, Distributions, LinearAlgebra, Plots, Statistics, Printf"
  },
  {
    "objectID": "functions.html#discretization",
    "href": "functions.html#discretization",
    "title": "Key Functions",
    "section": "Discretization",
    "text": "Discretization\nThe relevant code files for this step are tauchen.jl and makegrids.jl.\nRecall that the process that governs the evolution of shock \\(z\\) is given as: \\[\n    \\ln(z') = \\rho \\ln(z) + \\sigma \\varepsilon', \\quad \\varepsilon' \\sim \\mathcal{N}(0, 1)\n\\]\nUsing the Tauchen method, we can approximate this continuous state space with a discrete Markov chain. The following function implements the Tauchen method to generate a grid of \\(z\\) values and the corresponding transition probabilities. The input parameters for this function are: the autocorrelation œÅ = 0.7, intercept of the AR1 Œº = 0, standard deviation of the residual œÉ = 0.2, number of standard deviations q = 3, and the number of grid points N_z = 41.\n\n\nCode\nfunction tauchen(mew::Float64, sigma::Float64, rho::Float64, znum::Int, q::Float64)\n\n    zstar = mew / (1.0 - rho) #expected value of z\n    sigmaz = sigma / sqrt(1.0 - rho^2) #stddev of z\n\n    z = zstar .+ collect(range(-q * sigmaz, stop=q * sigmaz, length=znum))\n\n\n    trans = zeros(znum, znum)\n    w = (z[2] - z[1])  #Note that all the points are equidistant by construction.\n    for iz in 1:znum\n        for izz in 1:znum\n            binhi = (z[izz] - rho * z[iz] + w / 2.0) / sigma\n            binlo = (z[izz] - rho * z[iz] - w / 2.0) / sigma\n            if izz == 1\n                trans[iz, izz] = cdf(Normal(), binhi)\n            elseif izz == znum\n                trans[iz, izz] = 1.0 - cdf(Normal(), binlo)\n            else\n                trans[iz, izz] = cdf(Normal(), binhi) - cdf(Normal(), binlo)\n            end\n        end\n    end\n\n    return z::Vector{Float64}, trans::Matrix{Float64}\nend\n\n\nThe output is a vector of \\(z\\) values and a matrix of transition probabilities. Throughout the code, we refer to them as z_grid and trans, respectively.\n\n\nCode\n# Tauchen parameters\nN_z = 41   # shock grid number\nœÉ = 0.20   # SD of error term\nŒº = 0.0    # AR1 intercept\nœÅ = 0.70   # AR1 coefficient\nq = 3.0    # 3 standard deviations for the z\n\nz_grid, trans = tauchen(Œº, œÉ, œÅ, N_z, q);\nz_grid = exp.(z_grid);\n\n\nWe discretize the \\(z\\) shock to 41 points.\n\nprint(z_grid)\n\n[0.4316379803912925, 0.45015664900394275, 0.46946982853260255, 0.48961160607115856, 0.5106175311603266, 0.5325246785313611, 0.5553717135416735, 0.579198960417851, 0.6040484734265217, 0.6299641110986802, 0.656991613638479, 0.6851786836531056, 0.7145750703462349, 0.7452326573236532, 0.7772055541660273, 0.8105501919304436, 0.8453254227492717, 0.8815926237021424, 0.9194158051443708, 0.9588617236830193, 1.0, 1.0429032417301707, 1.0876471716112988, 1.1343107611320749, 1.182976369914058, 1.2337298910735608, 1.2866609028200269, 1.3418628265584742, 1.3994330917750424, 1.4594733079966673, 1.52208944412838, 1.5873920154847612, 1.6554962788456467, 1.7265224358803595, 1.8005958452994977, 1.877847244108723, 1.9584129783550546, 2.0424352437729247, 2.1300623367547344, 2.2214489160888555, 2.316756275927041]\n\n\nThe transition matrix is a 41 x 41 matrix, where element \\((i,j)\\) represents the probability of moving from state \\(z_i\\) to state \\(z_j\\).\n\nsize(trans)\n\n(41, 41)\n\n\nNext, we discretize the capital stock space, so that later on, our policy function contains integers indexing different \\(K\\) values. The upper and lower bounds on \\(K\\) are obtained by plugging in the minimum and maximum values of \\(z\\) into the steady state equation. Note that the Euler equation is given as: \\[ K = (\\frac{r+Œ¥}{Œ±z})^{1-Œ±}\\] which provides solutions to the equation that sets the marginal product of capital equal to the user cost of capital.\nWe discretize \\(K\\) into 501 points. Here is an example from the valfunclunky code script, where the risk free rate rate is 4%, capital depreciation rate Œ¥ = 0.15, curvature parameter Œ± = 0.70, and equity adjustment cost Œª = 0.1.\n\n\nCode\n# Dimensions\nN_k = 501\nN_k_pol = 501\n\n# Risk free rate and discount factor\nrf = 0.04\nŒ≤ = 1.0 / (1.0 + rf)\n\n# Parameters\nŒ¥ = 0.15   # capital depreciation\nŒ± = 0.70   # curvature parameter\nŒª = 0.1    # equity adjustment cost\n\n##### Discretize capital \nzmax = maximum(z_grid)\nzmin = minimum(z_grid)\nskale = 0.2 # to make steady state capital stock reasonable\n\n# Euler: plug in min & max of z to get bounds on k\nkminstar = ((rf + Œ¥) / (Œ± * skale * zmin))^(1.0 / (Œ± - 1.0))\nkminstar = log(kminstar)\n\nkmaxstar = ((rf + Œ¥) / (Œ± * skale * zmax))^(1.0 / (Œ± - 1.0))\nkmaxstar = log(kmaxstar)\n\n# Create k grid (and take exponents back)\nk_grid = collect(range(kminstar, stop=kmaxstar, length=N_k))\nk_grid = exp.(k_grid)\n\n# Create k' grid (policy grid)\nk_pol_grid = collect(range(kminstar, stop=kmaxstar, length=N_k_pol))\nk_pol_grid = exp.(k_pol_grid);\n\nprintln(\"Length of the policy grid is \", length(k_pol_grid))\n\n\nLength of the policy grid is 501"
  },
  {
    "objectID": "functions.html#profit-function",
    "href": "functions.html#profit-function",
    "title": "Key Functions",
    "section": "Profit Function",
    "text": "Profit Function\nRecall that the firm‚Äôs objective is to maximize the present value of cash flows to shareholders. For convenience, let‚Äôs just call this the profit function. As already discussed in the Model section, the profit function \\(E(\\cdot)\\) is defined piecewise according to the sign of internal cash flows \\(E*(\\cdot)\\):\n\\[\n\\begin{align*}\n    E(K, K', z) =\n    \\begin{cases}\n    E^* & \\text{if } E^* \\geq 0 \\\\\n    E^*(1 + \\lambda) & \\text{if } E^* &lt; 0\n    \\end{cases}\n\\end{align*}\n\\] where \\[\n    E^*(K, K', z) = zK^{\\alpha} - (K' - (1 - \\delta)K).\n\\]\nThe profit function has three input parameters: current capital stock K, next period‚Äôs capital stock K', and profitability shock z. Since these variables have all been discretized, we can fully capture the profit function in a 3D array. The following code fills in this array with the profit function values for different (z, K, K') combinations.\n\n\nCode\n# Initialize profit array\nprofit = zeros(N_z, N_k, N_k_pol)\n\n# Loop over N_k_pol = 501 values of K' ...\nThreads.@threads for ik_pol in 1:N_k_pol\n\n    # ... and N_k = 501 values of K\n    for ik in 1:N_k\n\n        # ... and N_z = 41 values of z\n        for iz in 1:N_z\n\n            # Profit = internal cash flow if positive\n            profit[iz, ik, ik_pol] =\n                skale * z_grid[iz] * k_grid[ik]^Œ± -\n                (k_pol_grid[ik_pol] - (1.0 - Œ¥) * k_grid[ik])\n\n            # If negative, pay financing costs Œª%    \n            if profit[iz, ik, ik_pol] &lt; 0.0\n                profit[iz, ik, ik_pol] =\n                    profit[iz, ik, ik_pol] * (1.0 + Œª)\n            end\n        end\n    end\nend\n\nprintln(\"Size of the profit array is \", size(profit))\n\n\nSize of the profit array is (41, 501, 501)\n\n\n\n\nA nested parallel loop likely causes too much overhead, so I only parallelize the outer loop here."
  },
  {
    "objectID": "functions.html#bellman-equation",
    "href": "functions.html#bellman-equation",
    "title": "Key Functions",
    "section": "Bellman Equation",
    "text": "Bellman Equation\nNext, we formulate the Bellman equation and numerically solve the maximization problem. The key relevant files are valfun.jl and maxbellman.jl, and there are a few more files that implement more advanced computational methods. inbetween.jl is for interpolation in a set-up where policy grid is finer than the capital grid (i.e.¬†N_k_pol &gt; N_k). howard.jl implements Howard‚Äôs policy improvement algorithm, which is a more efficient way to solve the Bellman equation in earlier iterations of the VFI and when the policy function has already converged. update.jl uses McQueen-Porteus algorithm to update the policy and value functions. More details can be found in the summer school slides.\nThe core of the VFI is to keep updating the value function until it converges to a numerical maximum. To do so, we keep iterating on the Bellman equation until the maximum difference between the current and previous value function is less than a given tolerance level. So, at each iteration, we are really comparing the best value function we have found so far, against the next candidate maximum.\nThe criter"
  },
  {
    "objectID": "model.html",
    "href": "model.html",
    "title": "The Model",
    "section": "",
    "text": "The setting is an infinite-horizon, discrete-time environment in which a manager acts on behalf of shareholders to maximize the expected present value of their distributions. The firm uses capital in a decreasing-returns technology to generate operating income according to \\(zK^{\\alpha}\\), where \\(K\\) is the capital stock, \\(z\\) is a profitability shock, and \\(\\alpha &lt; 1\\) governs the degree of returns to scale.\nThe profitability shock \\(z\\) is log-normally distributed and follows the process: \\[\n    \\ln(z') = \\rho \\ln(z) + \\sigma \\varepsilon', \\quad \\varepsilon' \\sim \\mathcal{N}(0, 1)\n\\]\nEach period, the firm chooses investment \\(I\\), which is defined by a standard capital accumulation identity: \\[\nK' = (1 - \\delta)K + I\n\\]\nThe price of capital goods is normalized to one. The firm‚Äôs cash flow, \\(E^*(K, K', z)\\), is its operating income minus its expenditure on investment:\n\n\\[\n    \\underbrace{E^*(K, K', z)}_{\\text{Internal cash flow}} = \\underbrace{zK^{\\alpha}}_{\\text{Income}} - \\underbrace{(K' - (1 - \\delta)K)}_{\\text{Investment}}\n\\]\nCash flows to shareholders, \\(E(K, K', z)\\), are defined in terms of the firm‚Äôs (internal) cash flows \\(E^*(K, K', z)\\). A positive firm cash flow is distributed to its stockholders, while a negative cash flow implies that the firm instead obtains funds from shareholders. In this case, the firm pays a linear cost \\(\\lambda\\). Thus:\n\n\nThe \\(E\\) function can also be thought of as the firm‚Äôs per-period return function.\n\\[\n\\begin{align*}\n    E(K, K', z) =\n    \\begin{cases}\n    E^* & \\text{if } E^* \\geq 0 \\\\\n    E^*(1 + \\lambda) & \\text{if } E^* &lt; 0\n    \\end{cases}\n\\end{align*}\n\\]\nHaving defined cash flows, we can now state the firm‚Äôs dynamic programming problem: \\[\n\\begin{aligned}\n    \\Pi(K, z) = & \\max_{K'} \\left\\{ E(K, K', z) + \\beta \\; \\mathbb{E}[\\Pi(K', z')] \\right\\} \\\\\n            &\\text{s.t.} \\quad K' = (1 - \\delta)K + I\n\\end{aligned}\n\\]\nNotice that this optimization problem does not have a closed-form solution, because of the non-linearity introduced by the cash flow function \\(E(K, K', z)\\). Therefore, we will solve the problem numerically using value function iteration."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Wharton 2025 Summer School",
    "section": "",
    "text": "This site contains my solutions to the project assignment from the Wharton summer school.\nThe course was taught by Toni Whited, Luke Taylor, and Stephen Terry.\n\nüóìÔ∏è Program Schedule\nUse the top menu bar to navigate to different sections:\n\nModel describes the dynamic model we are estimating.\nProject provides the project instructions and desired deliverables for each day.\nFunctions contains the key functions used in the estimation"
  },
  {
    "objectID": "day2.html",
    "href": "day2.html",
    "title": "Day 2 - Value Function Iteration",
    "section": "",
    "text": "Goals\nThe goals for this day to develop an identification strategy, choose features of the data to target in the estimation, and write code for computing those features in both the actual and simulated data. Set \\(\\beta = 0.96\\), \\(\\rho = 0.75\\), \\(\\sigma = 0.30\\), and \\(\\lambda = 0.05\\). The deliverables are:\n\nWhich features of the data will you ask the model to fit? These features may include means, variances, regression coefficients, or correlations.\nExplain which moment or moments are most important for identifying each model parameter. Create a table showing how each simulated moment changes as you perturb \\(\\alpha\\) and \\(\\delta\\).\nDownlad the data and read the documentation. The data set contains cleaned Compustat data on non-financial firms from 1971 - 2016. These variables should give you a hint about the types of moments that will work well.\nWrite a subroutine for computing the vector of moments from the actual data and simulated data.\nIf one of your moments is an AR1 coefficient, use the Han and Philips method at the end of Toni‚Äôs slides from today.\n\n\n\nMoment Selection\n\n\nSensitivity of Moments\n\n\nEmpirical Data"
  },
  {
    "objectID": "day1.html",
    "href": "day1.html",
    "title": "Day 1 - Value Function Iteration",
    "section": "",
    "text": "Goals\nThe goals for this day are to numerically solve the model and understand how it works, using the numerical techniques covered by Stephen Terry.\n\n\nDeliverables\n\nPlot the value function against the state variables. What is the intuition?\nPlot the policy function against the state variables. What is the intuition?\nHow do the two plots above change with the values of \\(\\alpha\\) and \\(\\delta\\)?\nWrite a subroutine that takes the model‚Äôs solution and parameters as inputs, then produces a simulated data set as output.\nUse the Tauchen (1986) method to discretize the state space.\n\n\n\nComparative Statics\n\n\nSensitivity Analysis"
  }
]